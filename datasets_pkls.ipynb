{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "3dc01ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fdd6b6",
   "metadata": {},
   "source": [
    "# alzheimer data and its training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8ca814e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_pipeline_alzheimer.pkl']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_alzheimer = \"https://drive.google.com/file/d/1JSchS-yltPrM6Jp_1U76-uv-QS8kc-hA/view?usp=share_link\"\n",
    "path_alzheimer = 'https://drive.google.com/uc?export=download&id='+url_alzheimer.split('/')[-2]\n",
    "alzheimer = pd.read_csv(path_alzheimer)\n",
    "\n",
    "# For those who considered to be 'Converted' will be regarded as 'Demented'\n",
    "alzheimer.loc[alzheimer[\"Group\"] == \"Converted\", \"Group\"] = \"Demented\"\n",
    "\n",
    "alzheimer.rename(columns = {\"Group\": \"alzheimer\", \"M/F\": \"sex\", \"Age\":\"age\", \"EDUC\": \"years_of_education\",\n",
    "                            \"SES\": \"socioeconomic_status\", \"MMSE\": \"mental_state_examination\", \n",
    "                            \"CDR\": \"clinical_dementia_rating\", \"eTIV\": \"intracranial_volume\", \n",
    "                            \"nWBV\": \"norm_brain_volume\", \"ASF\": \"atlas_scaling\" }, inplace = True)\n",
    "\n",
    "alzheimer.alzheimer = alzheimer.alzheimer.map({\"Demented\": 1,\"Nondemented\": 0})\n",
    "alzheimer.to_csv(\"final_alzheimer.csv\", index = False)\n",
    "# X and y creation\n",
    "X = alzheimer\n",
    "y = alzheimer.pop(\"alzheimer\")\n",
    "\n",
    "# data splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "X_train.head()\n",
    "\n",
    "ord_cols = [\"socioeconomic_status\", \"mental_state_examination\", \"clinical_dementia_rating\"]\n",
    "\n",
    "qualities1 = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "qualities2 = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0,\n",
    "              11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0,\n",
    "              21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0]\n",
    "qualities3 = [0.0, 0.5, 1.0, 2.0, 3.0]\n",
    "\n",
    "ord_cols_categories = [qualities1] + [qualities2] + [qualities3]\n",
    "\n",
    "cat_cols = [\"sex\"]\n",
    "\n",
    "num_cols = (\n",
    "    X_train\n",
    "    .drop(columns=ord_cols + cat_cols)\n",
    "    .columns)\n",
    "\n",
    "# ordinal pipeline\n",
    "ord_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(categories=ord_cols_categories,\n",
    "                               handle_unknown=\"use_encoded_value\",\n",
    "                               unknown_value=-1))\n",
    "])\n",
    "\n",
    "# nominal pipeline\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(fill_value=\"missing\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))\n",
    "])\n",
    "\n",
    "# numeric pipeline\n",
    "num_pipe = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer()),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "        ('ord', ord_pipe, ord_cols),\n",
    "        ('nom', cat_pipe, cat_cols),\n",
    "        ('num', num_pipe, num_cols)\n",
    "])\n",
    "\n",
    "reg = LogisticRegression(solver=\"liblinear\", C=10.0, max_iter=1000, random_state=0)\n",
    "\n",
    "model_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    #('var_threshold', VarianceThreshold()),\n",
    "    #('mod_feature_sel', SelectFromModel(RandomForestRegressor())),\n",
    "    # apply different models here\n",
    "    ('regressor', reg)\n",
    "])\n",
    "\n",
    "model_pipe.fit(X_train, y_train)\n",
    "joblib.dump(model_pipe, 'model_pipeline_alzheimer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c947fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed343b8e",
   "metadata": {},
   "source": [
    "# Lung cancer data and its training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "86b2d090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_pipeline_lung_cancer.pkl']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_lungcancer= \"https://drive.google.com/file/d/1_MTIc5RM4zA3b9G1KIx3Qhbg2dy9wk8u/view?usp=share_link\"\n",
    "path_lungcancer = 'https://drive.google.com/uc?export=download&id='+url_lungcancer.split('/')[-2]\n",
    "lung_cancer = pd.read_csv(path_lungcancer)\n",
    "\n",
    "# import the dataset\n",
    "#lung_cancer = pd.read_csv('/Users/baeyeeun/Desktop/DataScienceBootCamp/fianl_project/LungCancer.csv')\n",
    "\n",
    "# Drop NA values\n",
    "lung_cancer.dropna(inplace=True) # no missing values\n",
    "\n",
    "# Convert the LUNG_CANCER values into binary -- NO: False, YES: True\n",
    "lung_cancer[\"LUNG_CANCER\"] = lung_cancer[\"LUNG_CANCER\"].map({\"YES\":True, \"NO\":False})\n",
    "\n",
    "# Convert symptom variable values into binary -- 1:0, 2:1 so that it could be recognized as True/False values\n",
    "#lung_cancer.iloc[:,2:-2] = lung_cancer.iloc[:,2:-2].replace({1:0, 2:1})\n",
    "\n",
    "# Make age into classes by decade\n",
    "#lung_cancer['AGE']= pd.cut(lung_cancer.AGE, bins=[0,10,20,30,40,50,60,70,80,90,100], labels=['under_10', '10s', '20s', '30s', '40s', '50s', '60s', '70s', '80s', '90s'])\n",
    "\n",
    "lung_cancer.columns= lung_cancer.columns.str.lower()\n",
    "lung_cancer.columns = lung_cancer.columns.str.replace(' ', '_')\n",
    "lung_cancer.rename(columns = {\"gender\": \"sex\"},inplace = True)\n",
    "lung_cancer.to_csv(\"final_lung_cancer.csv\",index = False)\n",
    "lung_cancer = pd.read_csv(\"final_lung_cancer.csv\")\n",
    "# X and y creation\n",
    "X = lung_cancer\n",
    "y = lung_cancer.pop(\"lung_cancer\")\n",
    "\n",
    "# data splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "X_train.head()\n",
    "\n",
    "num_cols = [\"age\"]\n",
    "\n",
    "cat_cols = (\n",
    "    X_train\n",
    "    .drop(columns=num_cols)\n",
    "    .columns)\n",
    "\n",
    "# nominal pipeline\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(fill_value=\"missing\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))\n",
    "])\n",
    "\n",
    "# numeric pipeline\n",
    "num_pipe = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer()),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "        ('nom', cat_pipe, cat_cols),\n",
    "        ('num', num_pipe, num_cols)\n",
    "])\n",
    "\n",
    "reg = LogisticRegression(solver=\"liblinear\", C=10.0, max_iter=1000, random_state=0)\n",
    "\n",
    "model_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    #('var_threshold', VarianceThreshold()),\n",
    "    #('mod_feature_sel', SelectFromModel(RandomForestRegressor())),\n",
    "    # apply different models here\n",
    "    ('regressor', reg)\n",
    "])\n",
    "\n",
    "model_pipe.fit(X_train, y_train)\n",
    "joblib.dump(model_pipe, 'model_pipeline_lung_cancer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d90c69",
   "metadata": {},
   "source": [
    "# Heart data and its training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "cedfef31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_pipeline_HeartDisease.pkl']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_heartdisease = \"https://drive.google.com/file/d/1ZKgoOAmtinjh6MCVjBvf3Q67ISrF5GBe/view?usp=share_link\"\n",
    "path_heartdisease = 'https://drive.google.com/uc?export=download&id='+url_heartdisease.split('/')[-2]\n",
    "final_HeartDisease = pd.read_csv(path_heartdisease)\n",
    "'''\n",
    "# use get_dummies to convert the string column to int\n",
    "sex = pd.get_dummies(df['Sex'], prefix=\"Sex\", drop_first=True)\n",
    "chestpaintype = pd.get_dummies(df['ChestPainType'], prefix=\"cpt\", drop_first=True)\n",
    "restingecg = pd.get_dummies(df['RestingECG'], prefix=\"ECG\", drop_first=True)\n",
    "exerciseangina = pd.get_dummies(df['ExerciseAngina'], prefix=\"angina\", drop_first=True)\n",
    "st_slope = pd.get_dummies(df['ST_Slope'], prefix=\"ST\", drop_first=True)\n",
    "\n",
    "# drop the columns applied dummies then use concat\n",
    "df.drop(columns=['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope'], inplace=True)\n",
    "df = pd.concat([df, sex, chestpaintype, restingecg, exerciseangina, st_slope], axis=1)\n",
    "\n",
    "'''\n",
    "# X and y creation\n",
    "\n",
    "final_HeartDisease.rename(columns = {\"Age\": \"age\", \"Sex\": \"sex\"}, inplace = True)\n",
    "\n",
    "final_HeartDisease.to_csv(\"final_HeartDisease.csv\", index = False)\n",
    "X = final_HeartDisease\n",
    "y = final_HeartDisease.pop(\"HeartDisease\")\n",
    "\n",
    "# data splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "X_train.head()\n",
    "\n",
    "ord_cols = [\"ST_Slope\"]\n",
    "qualities = [\"Down\", \"Flat\", \"Up\"]\n",
    "ord_cols_categories = [qualities]\n",
    "cat_cols = [\"sex\", \"ChestPainType\", \"FastingBS\", \"RestingECG\", \"ExerciseAngina\"]\n",
    "num_cols = (X_train.drop(columns=ord_cols + cat_cols).columns)\n",
    "\n",
    "ord_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(categories=ord_cols_categories,\n",
    "                               handle_unknown=\"use_encoded_value\",\n",
    "                               unknown_value=-1))\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(fill_value=\"missing\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))\n",
    "])\n",
    "\n",
    "num_pipe = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer()),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "        ('ord', ord_pipe, ord_cols),\n",
    "        ('nom', cat_pipe, cat_cols),\n",
    "        ('num', num_pipe, num_cols)\n",
    "])\n",
    "\n",
    "reg = LogisticRegression(solver=\"liblinear\", C=10.0, max_iter=1000, random_state=0)\n",
    "\n",
    "model_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    #('var_threshold', VarianceThreshold()),\n",
    "    #('mod_feature_sel', SelectFromModel(RandomForestRegressor())),\n",
    "    # apply different models here\n",
    "    ('regressor', reg)\n",
    "])\n",
    "\n",
    "model_pipe.fit(X_train, y_train)\n",
    "joblib.dump(model_pipe, 'model_pipeline_HeartDisease.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf14704",
   "metadata": {},
   "source": [
    "# stroke data and its training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "3a8779d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_pipeline_Stroke.pkl']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_stroke = \"https://drive.google.com/file/d/1Kl6cTtJpMSJHwthPc1wR-06D3paapavb/view?usp=share_link\"\n",
    "path_stroke = 'https://drive.google.com/uc?export=download&id='+url_stroke.split('/')[-2]\n",
    "stroke = pd.read_csv(path_stroke)\n",
    "# source: https://www.kaggle.com/datasets/jillanisofttech/brain-stroke-dataset\n",
    "\n",
    "stroke['age'] = stroke['age'].astype(int)\n",
    "\n",
    "# Convert values in each column\n",
    "#stroke[\"gender\"] = stroke[\"gender\"].map({\"Female\":0, \"Male\":1})\n",
    "#stroke[\"ever_married\"] = stroke[\"ever_married\"].map({\"No\":0, \"Yes\":1})\n",
    "#stroke[\"Residence_type\"] = stroke[\"Residence_type\"].map({\"Rural\":0, \"Urban\":1})\n",
    "#stroke[\"work_type\"] = stroke[\"work_type\"].map({\"Private\": 0, \"Self-employed\": 1, \"children\": 2, \"Govt_job\": 3})\n",
    "#stroke[\"smoking_status\"] = stroke[\"smoking_status\"].map({\"never smoked\":0, \"Unknown\":0.5, \"formerly smoked\": 1, \"smokes\": 2})\n",
    "\n",
    "stroke.columns= stroke.columns.str.capitalize()\n",
    "stroke.rename(columns={'Gender':'sex', \"Age\": \"age\"}, inplace=True)\n",
    "stroke.sex = stroke.sex.map({\"Male\": \"M\",\"Female\": \"F\" })\n",
    "stroke.to_csv(\"final_Stroke.csv\", index = False)\n",
    "\n",
    "# X and y creation\n",
    "X = stroke\n",
    "y = stroke.pop(\"Stroke\")\n",
    "\n",
    "# data splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "num_cols = [\"age\", \"Avg_glucose_level\", \"Bmi\"]\n",
    "\n",
    "ord_cols = [\"Smoking_status\"]\n",
    "\n",
    "qualities = [\"never smoked\", \"formerly smoked\", \"Unknown\", \"smokes\"]\n",
    "ord_cols_categories = [qualities]\n",
    "\n",
    "cat_cols = [\"sex\"]\n",
    "\n",
    "cat_cols = (\n",
    "    X_train\n",
    "    .drop(columns=ord_cols + num_cols)\n",
    "    .columns)\n",
    "\n",
    "# ordinal pipeline\n",
    "ord_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(categories=ord_cols_categories,\n",
    "                               handle_unknown=\"use_encoded_value\",\n",
    "                               unknown_value=-1))\n",
    "])\n",
    "\n",
    "# nominal pipeline\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(fill_value=\"missing\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))\n",
    "])\n",
    "\n",
    "# numeric pipeline\n",
    "num_pipe = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer()),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "        ('ord', ord_pipe, ord_cols),\n",
    "        ('nom', cat_pipe, cat_cols),\n",
    "        ('num', num_pipe, num_cols)\n",
    "])\n",
    "\n",
    "reg = LogisticRegression(solver=\"liblinear\", C=10.0, max_iter=1000, random_state=0)\n",
    "model_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    #('var_threshold', VarianceThreshold()),\n",
    "    #('mod_feature_sel', SelectFromModel(RandomForestRegressor())),\n",
    "    # apply different models here\n",
    "    ('regressor', reg)\n",
    "])\n",
    "\n",
    "model_pipe.fit(X_train, y_train)\n",
    "joblib.dump(model_pipe, 'model_pipeline_Stroke.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110680f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
